import warnings
warnings.filterwarnings("ignore", message="xFormers is available")

import torch
import wandb
import hydra
import os
import numpy as np
from pathlib import Path

from tqdm import tqdm
from torch import nn
from torch.optim import Optimizer
from torch.utils.data import DataLoader
from hydra.core.config_store import ConfigStore
from omegaconf import OmegaConf

from data.datamodule import DataModule, BatchDict
from data.dataset import Dataset
from configs.experiments.base import BaseTrainConfig
from utils.sanity import show_images, check_initial_loss
from utils.validation import validate_and_log
from utils.transforms import TargetStandardizer

def train_single_fold(cfg: BaseTrainConfig, fold_idx: int, train_loader: DataLoader, 
                     val_loader: DataLoader, logger=None) -> dict:
    """è®­ç»ƒå•ä¸ªæŠ˜"""
    
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    
    # åˆ›å»ºæ¨¡å‹
    model = hydra.utils.instantiate(cfg.model.instance).to(device)
    optimizer = hydra.utils.instantiate(cfg.optim, params=model.parameters())
    loss_fn = hydra.utils.instantiate(cfg.loss_fn)
    
    # ç¡®å®šæ˜¯å¦ä½¿ç”¨æ ‡å‡†åŒ–
    target_standardizer = None
    if hasattr(cfg.datamodule, 'standardize_target') and cfg.datamodule.standardize_target:
        target_standardizer = TargetStandardizer(
            mu=cfg.datamodule.target_mu,
            sigma=cfg.datamodule.target_sigma
        )
        print(f"Fold {fold_idx}: ä½¿ç”¨ç›®æ ‡æ ‡å‡†åŒ– mu={cfg.datamodule.target_mu}, sigma={cfg.datamodule.target_sigma}")
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    fold_save_dir = f"{cfg.msle_validation_dir}/fold_{fold_idx}"
    os.makedirs(fold_save_dir, exist_ok=True)
    
    # ğŸ” æ£€æŸ¥åˆå§‹æŸå¤±ï¼ˆè®­ç»ƒå‰ï¼‰
    print(f"\nğŸ” Fold {fold_idx}: è®­ç»ƒå‰åˆå§‹æŸå¤±æ£€æŸ¥")
    print("-" * 40)
    
    initial_train_loss = check_initial_loss(
        model=model, 
        data_loader=train_loader, 
        loss_fn=loss_fn, 
        device=device, 
        name=f"Fold {fold_idx} è®­ç»ƒé›†"
    )
    
    initial_val_loss = check_initial_loss(
        model=model, 
        data_loader=val_loader, 
        loss_fn=loss_fn, 
        device=device, 
        name=f"Fold {fold_idx} éªŒè¯é›†"
    )
    
    # è®°å½•åˆå§‹æŸå¤±åˆ°wandb
    if logger is not None:
        logger.log({
            f"fold_{fold_idx}/initial_loss/train": initial_train_loss,
            f"fold_{fold_idx}/initial_loss/val": initial_val_loss,
            "fold": fold_idx,
        })
    
    print(f"Fold {fold_idx} åˆå§‹æŸå¤± - è®­ç»ƒ: {initial_train_loss:.4f}, éªŒè¯: {initial_val_loss:.4f}")
    print("-" * 40)
    
    best_val_loss = float('inf')
    fold_metrics = {
        'fold': fold_idx,
        'initial_train_loss': initial_train_loss,
        'initial_val_loss': initial_val_loss,
        'train_losses': [],
        'val_losses': [],
        'msle_scores': []
    }
    
    # è®­ç»ƒå¾ªç¯
    for epoch in tqdm(range(cfg.epochs), desc=f"Fold {fold_idx} Epochs"):
        # è®­ç»ƒé˜¶æ®µ
        model.train()
        epoch_train_loss = 0
        num_samples_train = 0
        
        pbar = tqdm(train_loader, desc=f"Fold {fold_idx} Epoch {epoch}", leave=False)
        for i, batch in enumerate(pbar):
            batch: BatchDict = batch
            batch["image"] = batch["image"].to(device)
            batch["target"] = batch["target"].to(device).squeeze()
            
            # å¦‚æœbatchä¸­æœ‰age_yearç‰¹å¾ï¼Œä¹Ÿç§»åŠ¨åˆ°è®¾å¤‡ä¸Š
            if "age_year" in batch:
                batch["age_year"] = batch["age_year"].to(device)
            
            preds = model(batch).squeeze()
            loss = loss_fn(preds, batch["target"])
            
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            
            # è®°å½•èåˆæƒé‡ï¼ˆå¦‚æœæ¨¡å‹æ”¯æŒï¼‰
            if hasattr(model, 'get_fusion_weights') and logger is not None:
                fusion_weights = model.get_fusion_weights()
                log_dict = {
                    f"fold_{fold_idx}/train/loss_step": loss.detach().cpu().numpy(),
                    "fold": fold_idx,
                    "epoch": epoch
                }
                # è®°å½•æ‰€æœ‰èåˆæƒé‡
                for weight_name, weight_value in fusion_weights.items():
                    log_dict[f"fold_{fold_idx}/train/{weight_name}"] = weight_value
                logger.log(log_dict)
            elif hasattr(model, 'get_fusion_weight') and logger is not None:
                # å‘åå…¼å®¹ï¼šå¯¹äºåªæœ‰ä¸¤æ¨¡æ€çš„æ—§æ¨¡å‹
                logger.log({
                    f"fold_{fold_idx}/train/fusion_weight": model.get_fusion_weight(),
                    f"fold_{fold_idx}/train/loss_step": loss.detach().cpu().numpy(),
                    "fold": fold_idx,
                    "epoch": epoch
                })
            
            epoch_train_loss += loss.detach().cpu().numpy() * len(batch["image"])
            num_samples_train += len(batch["image"])
            pbar.set_postfix({"loss": loss.detach().cpu().numpy()})
            
        epoch_train_loss /= num_samples_train
        fold_metrics['train_losses'].append(epoch_train_loss)
        
        # éªŒè¯é˜¶æ®µ
        model.eval()
        epoch_val_loss = 0
        num_samples_val = 0
        
        with torch.no_grad():
            for batch in val_loader:
                batch: BatchDict = batch
                batch["image"] = batch["image"].to(device)
                batch["target"] = batch["target"].to(device).squeeze()
                
                # å¦‚æœbatchä¸­æœ‰age_yearç‰¹å¾ï¼Œä¹Ÿç§»åŠ¨åˆ°è®¾å¤‡ä¸Š
                if "age_year" in batch:
                    batch["age_year"] = batch["age_year"].to(device)
                
                preds = model(batch).squeeze()
                loss = loss_fn(preds, batch["target"])
                
                epoch_val_loss += loss.detach().cpu().numpy() * len(batch["image"])
                num_samples_val += len(batch["image"])
                
        epoch_val_loss /= num_samples_val
        fold_metrics['val_losses'].append(epoch_val_loss)
        
        # MSLE éªŒè¯
        if cfg.msle_validation_interval > 0 and ((epoch + 1) % cfg.msle_validation_interval == 0 or epoch == cfg.epochs - 1):
            msle, _ = validate_and_log(
                model=model,
                val_loader=val_loader,
                device=device,
                epoch=epoch,
                target_standardizer=target_standardizer,
                experiment_name=f"{cfg.experiment_name}_fold_{fold_idx}",
                save_dir=fold_save_dir,
                log_wandb=False,  # ä¸åœ¨è¿™é‡Œè®°å½•ï¼Œç»Ÿä¸€åœ¨å¤–é¢è®°å½•
                logger=None
            )
            fold_metrics['msle_scores'].append(msle)
            
            # è®°å½•åˆ° wandb
            if logger is not None:
                logger.log({
                    f"fold_{fold_idx}/train/loss_epoch": epoch_train_loss,
                    f"fold_{fold_idx}/val/loss_epoch": epoch_val_loss,
                    f"fold_{fold_idx}/val/msle": msle,
                    "fold": fold_idx,
                    "epoch": epoch
                })
        else:
            if logger is not None:
                logger.log({
                    f"fold_{fold_idx}/train/loss_epoch": epoch_train_loss,
                    f"fold_{fold_idx}/val/loss_epoch": epoch_val_loss,
                    "fold": fold_idx,
                    "epoch": epoch
                })
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if epoch_val_loss < best_val_loss:
            best_val_loss = epoch_val_loss
            os.makedirs("checkpoints", exist_ok=True)
            checkpoint_path = f"checkpoints/{cfg.model.name}_fold_{fold_idx}_best.pt"
            torch.save(model.state_dict(), checkpoint_path)
            fold_metrics['best_checkpoint'] = checkpoint_path
    
    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    os.makedirs("checkpoints", exist_ok=True)
    final_checkpoint_path = f"checkpoints/{cfg.model.name}_fold_{fold_idx}_final.pt"
    torch.save(model.state_dict(), final_checkpoint_path)
    fold_metrics['final_checkpoint'] = final_checkpoint_path
    fold_metrics['best_val_loss'] = best_val_loss
    
    return fold_metrics

@hydra.main(config_path="configs", config_name=None, version_base="1.3")
def train_kfold(cfg: BaseTrainConfig) -> None:
    """å¤šæ¨¡å‹è®­ç»ƒ - ä½¿ç”¨å›ºå®š90/10åˆ†å‰²ï¼Œä¸åŒéšæœºç§å­"""
    
    # è®¾ç½®è®­ç»ƒå‚æ•° - ä½¿ç”¨90/10åˆ†å‰²è®­ç»ƒå¤šä¸ªæ¨¡å‹
    num_models = 5  # è®­ç»ƒ5ä¸ªæ¨¡å‹ï¼Œæ¯ä¸ªä½¿ç”¨ä¸åŒéšæœºç§å­
    val_split = 0.1  # 10%éªŒè¯ï¼Œ90%è®­ç»ƒ
    
    logger = (
        wandb.init(project="challenge_CSC_43M04_EP", name=f"{cfg.experiment_name}_multi_seed")
        if cfg.log
        else None
    )
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    save_dir = cfg.msle_validation_dir
    os.makedirs(save_dir, exist_ok=True)
    
    # è®°å½•é…ç½®
    if logger is not None:
        config_dict = OmegaConf.to_container(cfg, resolve=True)
        config_dict['num_models'] = num_models
        config_dict['val_split'] = val_split
        logger.config.update(config_dict)
    
    # å­˜å‚¨æ‰€æœ‰æ¨¡å‹çš„ç»“æœ
    all_model_metrics = []
    
    # è®­ç»ƒæ¯ä¸ªæ¨¡å‹
    for model_idx in range(num_models):
        print(f"\n{'='*50}")
        print(f"å¼€å§‹è®­ç»ƒæ¨¡å‹ {model_idx + 1}/{num_models} (ç§å­: {cfg.datamodule.seed + model_idx})")
        print(f"{'='*50}")
        
        # ä¸ºæ¯ä¸ªæ¨¡å‹ä½¿ç”¨ä¸åŒçš„éšæœºç§å­
        current_seed = cfg.datamodule.seed + model_idx
        
        # åˆ›å»ºæ•°æ®æ¨¡å—ï¼ˆä½¿ç”¨ä¸åŒç§å­ï¼‰
        datamodule_config = OmegaConf.to_container(cfg.datamodule, resolve=True)
        datamodule_config['seed'] = current_seed
        datamodule_config['val_split'] = val_split  # ç¡®ä¿ä½¿ç”¨90/10åˆ†å‰²
        base_datamodule = hydra.utils.instantiate(datamodule_config)
        
        # è·å–è®­ç»ƒå’ŒéªŒè¯åŠ è½½å™¨
        train_loader = base_datamodule.train_dataloader()
        val_loader = base_datamodule.val_dataloader()
        
        print(f"è®­ç»ƒé›†å¤§å°: {len(train_loader.dataset)}")
        print(f"éªŒè¯é›†å¤§å°: {len(val_loader.dataset)}")
        print(f"è®­ç»ƒæ•°æ®æ¯”ä¾‹: {len(train_loader.dataset) / (len(train_loader.dataset) + len(val_loader.dataset)) * 100:.1f}%")
        
        # è®­ç»ƒå½“å‰æ¨¡å‹
        model_metrics = train_single_fold(
            cfg=cfg,
            fold_idx=model_idx,
            train_loader=train_loader,
            val_loader=val_loader,
            logger=logger
        )
        
        all_model_metrics.append(model_metrics)
        
        print(f"æ¨¡å‹ {model_idx} å®Œæˆ:")
        print(f"  æœ€ä½³éªŒè¯æŸå¤±: {model_metrics['best_val_loss']:.4f}")
        if model_metrics['msle_scores']:
            print(f"  æœ€ç»ˆ MSLE: {model_metrics['msle_scores'][-1]:.4f}")
    
    # è®¡ç®—å¤šæ¨¡å‹ç»Ÿè®¡
    final_val_losses = [metrics['val_losses'][-1] for metrics in all_model_metrics]
    final_msle_scores = [metrics['msle_scores'][-1] if metrics['msle_scores'] else float('inf') 
                        for metrics in all_model_metrics]
    
    multi_model_val_loss_mean = np.mean(final_val_losses)
    multi_model_val_loss_std = np.std(final_val_losses)
    multi_model_msle_mean = np.mean([score for score in final_msle_scores if score != float('inf')])
    multi_model_msle_std = np.std([score for score in final_msle_scores if score != float('inf')])
    
    print(f"\n{'='*50}")
    print(f"å¤šæ¨¡å‹è®­ç»ƒç»“æœæ±‡æ€»:")
    print(f"{'='*50}")
    print(f"éªŒè¯æŸå¤±: {multi_model_val_loss_mean:.4f} Â± {multi_model_val_loss_std:.4f}")
    print(f"MSLE åˆ†æ•°: {multi_model_msle_mean:.4f} Â± {multi_model_msle_std:.4f}")
    
    # è®°å½•å¤šæ¨¡å‹æ±‡æ€»ç»“æœ
    if logger is not None:
        logger.log({
            "multi_model_summary/val_loss_mean": multi_model_val_loss_mean,
            "multi_model_summary/val_loss_std": multi_model_val_loss_std,
            "multi_model_summary/msle_mean": multi_model_msle_mean,
            "multi_model_summary/msle_std": multi_model_msle_std,
        })
        
        # è®°å½•æ¯ä¸ªæ¨¡å‹çš„æœ€ç»ˆç»“æœ
        for i, metrics in enumerate(all_model_metrics):
            logger.log({
                f"multi_model_final/model_{i}_val_loss": metrics['val_losses'][-1],
                f"multi_model_final/model_{i}_msle": metrics['msle_scores'][-1] if metrics['msle_scores'] else None,
            })
    
    # ä¿å­˜å¤šæ¨¡å‹è®­ç»ƒç»“æœ
    import json
    multi_model_results = {
        'num_models': num_models,
        'val_split': val_split,
        'multi_model_val_loss_mean': multi_model_val_loss_mean,
        'multi_model_val_loss_std': multi_model_val_loss_std,
        'multi_model_msle_mean': multi_model_msle_mean,
        'multi_model_msle_std': multi_model_msle_std,
        'model_metrics': all_model_metrics
    }
    
    with open(f"{save_dir}/multi_model_results.json", 'w') as f:
        json.dump(multi_model_results, f, indent=2)
    
    if cfg.log:
        logger.finish()
    
    print(f"\nå¤šæ¨¡å‹è®­ç»ƒå®Œæˆï¼ç»“æœå·²ä¿å­˜åˆ° {save_dir}/multi_model_results.json")

if __name__ == "__main__":
    import importlib
    import sys
    
    # Get config-name from command line arguments
    config_name = None
    for arg in sys.argv:
        if arg.startswith("--config-name="):
            config_name = arg.split("=")[1]
            break
    
    if config_name is None:
        raise ValueError("Please use --config-name to specify the config name")
    
    # Dynamically import config class
    try:
        config_module = importlib.import_module(f"configs.experiments.{config_name}")
        config_class = getattr(config_module, f"{config_name.capitalize()}TrainConfig")
    except (ImportError, AttributeError) as e:
        raise ImportError(f"Cannot load config {config_name}: {str(e)}")
    
    # Register config
    cs = ConfigStore.instance()
    cs.store(name=config_name, node=config_class)
    
    train_kfold() 